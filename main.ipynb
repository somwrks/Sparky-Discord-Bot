{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparky Discord Bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description:\n",
    "    \n",
    "This Discord bot uses the LangChain library to create a question-answering system.\n",
    "It uses the Hugging Face Hub to download pre-trained models and embeddings,\n",
    "and integrates with the Qdrant vector database for efficient search.\n",
    "The bot also supports multi-step reasoning, allowing users to ask questions\n",
    "that require multiple pieces of information from different sources. It also lists the citations used for the information\n",
    "\n",
    "The bot also supports natural language inference (NLI) using the\n",
    "Google Generative AI model. To use NLI, you must provide a\n",
    "question and two options, and the bot will generate a third option\n",
    "that is most likely to be the correct answer.\n",
    "\n",
    "The current use for this bot is to provide answers to questions regarding arizona state university \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "- The bot starts by connecting to the Qdrant vector database.\n",
    "- It then retrieves relevant documents from the database using the ASU University's search terms.\n",
    "- The bot uses the Hugging Face pipeline to generate answers based on the retrieved documents.\n",
    "- If a user asks a question that requires multi-step reasoning, the bot will generate a series of answers, each based on the previous one.\n",
    "- To handle natural language inference (NLI), the bot uses the Google Generative AI model.\n",
    "- The bot is designed to handle a variety of questions related to ASU University, such as academic information, campus life, and student life.\n",
    "\n",
    "![image](https://github.com/user-attachments/assets/6d79c439-ca05-4eed-ae1c-becc99e6cb37)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers\n",
    "# %pip install -U discord.py \n",
    "# %pip install nest_asyncio\n",
    "# %pip install langchain  llama-cpp-python langchain-qdrant langchain-huggingface  huggingface_hub google-generativeai\n",
    "# %pip install accelerate qdrant-client requests beautifulsoup4 discord.py chromadb sentence_transformers faiss-gpu redis aiohttp tenacity logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "\n",
    "We are using [gemini-1.5-flash](https://deepmind.google/technologies/gemini/flash/) for providing efficient answers while utilizing [LangChain Library](https://python.langchain.com/docs/introduction/) for managing agents along with [Beautiful Soup](https://beautiful-soup-4.readthedocs.io/en/latest/) for minimal webscraping support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import google.generativeai as genai\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_qdrant import Qdrant\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import logging\n",
    "from qdrant_client.models import Distance, VectorParams  # Add this\n",
    "from typing import List, Dict, Optional\n",
    "import concurrent.futures\n",
    "import os\n",
    "import tracemalloc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "tracemalloc.start()\n",
    "logger = logging.getLogger(__name__)\n",
    "os.environ['NUMEXPR_MAX_THREADS'] = '16'  # Or another appropriate number\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Web Scraping Class\n",
    "\n",
    "This class has methods to find relevant webpages and perform webscraping to gather raw data from websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASUWebScraper:\n",
    "    def __init__(self, base_domains: List[str], discord_client=None):\n",
    "        self.discord_client = discord_client\n",
    "        self.visited_urls = set()\n",
    "        self.text_content = []\n",
    "        genai.configure(api_key=api_key)  \n",
    "        self.model = genai.GenerativeModel('gemini-1.5-flash')  \n",
    "        self.base_domains = base_domains\n",
    "        self.headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Upgrade-Insecure-Requests': '1'\n",
    "    }\n",
    "    async def search_strategy(self, query: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Let Gemini decide which search method to use\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Analyze this query and decide which search method would be most appropriate:\n",
    "        Query: {query}\n",
    "        \n",
    "        Choose between:\n",
    "        1. Discord Search: Best for recent announcements, community discussions, or ASU-specific updates\n",
    "        2. Google Search: Best for general ASU information, academic content, or official documentation\n",
    "        \n",
    "        Respond with only: 'DISCORD' or 'GOOGLE' or 'BOTH'\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.model.generate_content(prompt)\n",
    "            decision = response.text.strip().upper()\n",
    "            \n",
    "            if decision == 'DISCORD':\n",
    "                return await self.search_discord_announcements(query)\n",
    "            elif decision == 'GOOGLE':\n",
    "                return self.google_search(query)\n",
    "            else:  # BOTH\n",
    "                discord_results = await self.search_discord_announcements(query)\n",
    "                google_results = self.google_search(query)\n",
    "                combined_results = discord_results + google_results\n",
    "                return combined_results\n",
    "\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Search strategy error: {str(e)}\")\n",
    "            return self.google_search(query)  # Fallback to Google search\n",
    "                \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text content.\"\"\"\n",
    "        import re\n",
    "        text = ' '.join(text.split())\n",
    "        text = re.sub(r'[^\\w\\s.,!?-]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip()\n",
    "\n",
    "    async def search_discord_announcements(self, query: str, limit: int = 2) -> List[Dict[str, str]]:\n",
    "        if not self.discord_client:\n",
    "            return []\n",
    "            \n",
    "        announcements_channel = discord.utils.get(\n",
    "            self.discord_client.get_all_channels(), \n",
    "            name='announcements'\n",
    "        )\n",
    "        \n",
    "        if not announcements_channel:\n",
    "            return []\n",
    "            \n",
    "        messages = []\n",
    "        async for message in announcements_channel.history(limit=100):\n",
    "            if query.lower() in message.content.lower():\n",
    "                messages.append({\n",
    "                    'url': f'discord://message/{message.id}',\n",
    "                    'content': message.content\n",
    "                })\n",
    "                if len(messages) >= limit:\n",
    "                    break\n",
    "        \n",
    "        print(\"discord messages\\n\\n\", messages)\n",
    "        return messages\n",
    "\n",
    "    def scrape_content(self, url: str) -> bool:\n",
    "        if url in self.visited_urls:\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            content_elements = soup.find_all([\n",
    "                'p', 'h1', 'h2', 'h3', 'li', 'td', 'th', \n",
    "                'table', 'div', 'span', 'article', 'section'\n",
    "            ])\n",
    "            \n",
    "            text = ' '.join([\n",
    "                self.clean_text(element.get_text())\n",
    "                for element in content_elements\n",
    "                if len(element.get_text().strip()) > 0\n",
    "            ])\n",
    "            \n",
    "            if text:\n",
    "                logger.info(f\"Extracted content from {url}:\\n{text[:100]}...\")\n",
    "                self.text_content.append({\n",
    "                    'url': url,\n",
    "                    'content': text\n",
    "                })\n",
    "                self.visited_urls.add(url)\n",
    "                return True\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error scraping {url}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "    def google_search(self, query: str) -> List[Dict[str, str]]:\n",
    "        domains = \"+OR+\".join([f\"site:{domain}\" for domain in self.base_domains])\n",
    "        google_query = query.lower().replace(\" \", \"+\")\n",
    "        search_url = f\"https://www.google.com/search?q={google_query}+({domains})\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(search_url, headers=self.headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            search_results = []\n",
    "            \n",
    "            for result in soup.find_all('div', class_='g'):\n",
    "                link = result.find('a')\n",
    "                if link and 'href' in link.attrs:\n",
    "                    url = link['href']\n",
    "                    if any(domain in url for domain in self.base_domains):\n",
    "                        search_results.append(url)\n",
    "            \n",
    "            search_results = search_results[:2]\n",
    "            \n",
    "            for url in search_results:\n",
    "                self.scrape_content(url)\n",
    "                \n",
    "            return self.text_content\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in google search: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    async def search(self, query: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Combined search method that aggregates results from both Google and Discord\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Get Google search results synchronously\n",
    "        google_results = await self.google_search(query)\n",
    "        results.extend(google_results)\n",
    "        \n",
    "        # Get Discord announcements asynchronously if client exists\n",
    "        if self.discord_client:\n",
    "            discord_results = await self.search_discord_announcements(query)\n",
    "            results.extend(discord_results)\n",
    "            \n",
    "        return results\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataPreProcessor Class\n",
    "\n",
    "This class preprocesses the web scraped data by cleaning it, splitting it into chunks, and preparing it for vector storage in a vector database like Qdrant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from typing import List, Dict, Optional\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=50\n",
    "        )\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "    async def process_documents(self, documents: List[Dict[str, str]], search_context: str) -> List[Document]:\n",
    "        try:\n",
    "            cleaned_docs = []\n",
    "            for doc in documents:\n",
    "                cleaned_text = ' '.join(doc['content'].split())\n",
    "                refined_text =  self._refine_with_gemini(search_context, cleaned_text)\n",
    "                if refined_text:\n",
    "                    cleaned_doc = Document(\n",
    "                        page_content=refined_text,\n",
    "                        metadata={'url': doc['url']}\n",
    "                    )\n",
    "                    cleaned_docs.append(cleaned_doc)\n",
    "            \n",
    "            splits = self.text_splitter.split_documents(cleaned_docs)\n",
    "            return splits\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing documents: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "\n",
    "    def _refine_with_gemini(self, search_context: str, text: str) -> Optional[str]:\n",
    "        prompt = f\"\"\" \n",
    "        You are a Data refiner. Refine and structure the following text to be more concise and informative, \n",
    "        while preserving all key information, keeping in mind with this context - {search_context}:\n",
    "        {text}\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.model.generate_content(prompt)\n",
    "            if response and hasattr(response, 'text'):\n",
    "                return response.text\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Gemini refinement error: {str(e)}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Som\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "INFO:datasets:PyTorch version 2.4.1 available.\n",
      "INFO:datasets:TensorFlow version 2.15.0 available.\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.base import Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class CustomEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name: str):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        \n",
    "    def embed_documents(self, documents: List[str]) -> List[List[float]]:\n",
    "        return [self.model.encode(d).tolist() for d in documents]\n",
    "        \n",
    "    def embed_query(self, query: str) -> List[float]:\n",
    "        return self.model.encode(query).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Vector DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use LLM here to refine the data stored to vector database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStoreManager:\n",
    "    @staticmethod\n",
    "    async def setup_vector_store(processed_docs: List[Document]) -> QdrantVectorStore:\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            model_kwargs={'device': 'cpu'}\n",
    "        )\n",
    "        \n",
    "        texts = []\n",
    "        metadatas = []\n",
    "        for doc in processed_docs:\n",
    "            if isinstance(doc, Document):\n",
    "                texts.append(doc.page_content)\n",
    "                metadatas.append(doc.metadata)\n",
    "                \n",
    "        return await QdrantVectorStore.afrom_texts(\n",
    "            texts=texts,\n",
    "            embedding=embeddings,\n",
    "            metadatas=metadatas,\n",
    "            collection_name=\"asu_docs\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Qdrant Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "class QdrantConnectionPool:\n",
    "    _instance = None\n",
    "    _lock = threading.Lock()\n",
    "\n",
    "    def __new__(cls):\n",
    "        with cls._lock:\n",
    "            if cls._instance is None:\n",
    "                cls._instance = super().__new__(cls)\n",
    "                cls._instance.client = None\n",
    "            return cls._instance\n",
    "\n",
    "    async def get_client(self):\n",
    "        if self.client is None:\n",
    "            embeddings = HuggingFaceEmbeddings(\n",
    "                model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "            )\n",
    "            self.client = await QdrantVectorStore(\n",
    "                client=QdrantClient(host=\"localhost\", port=6333),\n",
    "                collection_name=\"asu_docs\",\n",
    "                embedding=embeddings  # Changed from embeddings to embedding\n",
    "            )[3]\n",
    "        return self.client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "\n",
    "async def initialize_qdrant():\n",
    "    client =  QdrantClient(host=\"localhost\", port=6333)\n",
    "    \n",
    "    # Create collection if it doesn't exist\n",
    "    try:\n",
    "        \n",
    "        await client.create_collection(\n",
    "            collection_name=\"asu_docs\",\n",
    "            vectors_config=VectorParams(\n",
    "                size=384,  # Size for all-MiniLM-L6-v2 embeddings\n",
    "                distance=Distance.COSINE\n",
    "            )\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # Collection might already exist, which is fine\n",
    "        pass\n",
    "    \n",
    "    return client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the RAG Pipeline system\n",
    "\n",
    "Here we finally use all the classes and methods to get the final structure of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "class ASURagSystem:\n",
    "    def __init__(self, api_key: str, discord_client, initial_data=None,):\n",
    "        self.search_context =\"\"\n",
    "        self.api_key = api_key\n",
    "        self.scraper = ASUWebScraper(base_domains=[\n",
    "            \"asu.edu\", \"admission.asu.edu\", \"students.asu.edu\", \"degrees.asu.edu\",\n",
    "            \"catalog.asu.edu\", \"my.asu.edu\",\"thesundevils.com\", \"engineering.asu.edu\", \"business.asu.edu\",\n",
    "            \"clas.asu.edu\", \"thecollege.asu.edu\", \"design.asu.edu\", \"law.asu.edu\",\n",
    "            \"nursingandhealth.asu.edu\", \"education.asu.edu\", \"lib.asu.edu\",\n",
    "            \"graduate.asu.edu\", \"provost.asu.edu\", \"canvas.asu.edu\", \"tutoring.asu.edu\",\n",
    "            \"housing.asu.edu\", \"eoss.asu.edu\", \"career.asu.edu\", \"finance.asu.edu\",\n",
    "            \"scholarships.asu.edu\", \"research.asu.edu\", \"sustainability.asu.edu\",\n",
    "            \"biodesign.asu.edu\", \"polytechnic.asu.edu\", \"downtown.asu.edu\",\n",
    "            \"westcampus.asu.edu\", \"thunderbird.asu.edu\"\n",
    "        ], discord_client=discord_client)\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        model_kwargs={'device': 'cpu'}\n",
    "        )\n",
    "        self.vector_store = None\n",
    "        self.preprocessor =  DataPreprocessor(api_key=api_key)\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "        \n",
    "    async def initialize(self):\n",
    "        if self.vector_store is None:\n",
    "            client = await initialize_qdrant()\n",
    "            self.vector_store = QdrantVectorStore(\n",
    "                client=client,\n",
    "                collection_name=\"asu_docs\",\n",
    "                embedding=self.embeddings\n",
    "            )\n",
    "    def needs_web_search(self, question: str) -> bool:\n",
    "        prompt = \"\"\"\n",
    "        As an ASU information expert, analyze this question:\n",
    "        {question}\n",
    "        \n",
    "        Consider:\n",
    "        1. Is this about recent events/announcements? (Favor Discord)\n",
    "        2. Is this about general ASU information? (Favor Google)\n",
    "        3. Does this require both current and historical context? (Use both)\n",
    "        \n",
    "        Respond with only:\n",
    "        - 'NO_SEARCH': If you can answer without searching\n",
    "        - 'DISCORD': For recent/community information\n",
    "        - 'GOOGLE': For general ASU information\n",
    "        - 'BOTH': If both sources would be valuable\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.model.generate_content(prompt.format(question=question))\n",
    "            decision = response.text.strip().upper()\n",
    "            return decision != 'NO_SEARCH'\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error checking search necessity: {str(e)}\")\n",
    "            return True\n",
    "\n",
    "\n",
    "    def determine_search_context(self, question: str) -> str:\n",
    "        prompt = \"\"\"\n",
    "        As an ASU search context optimizer, your task is to convert the given question into a brief, focused search query that will help find relevant information from ASU websites.\n",
    "        \n",
    "        Guidelines:\n",
    "        - Keep the query concise (2-5 words)\n",
    "        - Focus on key topics and terms\n",
    "        - Remove unnecessary words\n",
    "        - Include \"ASU\" or relevant department names if needed\n",
    "        - Make it specific to ASU-related content\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Return only the search query, nothing else.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.model.generate_content(prompt.format(question=question))\n",
    "            search_context = response.text.strip()\n",
    "            self.search_context = search_context\n",
    "            logger.info(f\"Generated search context: {search_context}\")\n",
    "            return search_context\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating search context: {str(e)}\")\n",
    "            # Fallback to a simplified version of the question\n",
    "            return ' '.join(question.split()[:3])\n",
    "\n",
    "    async def initialize_system(self, query: str) -> None:\n",
    "        logger.info(\"Scraping ASU content matching query...\")\n",
    "        documents = await self.scraper.search(query)\n",
    "        if not documents:\n",
    "            raise ValueError(\"No documents found matching the query\")\n",
    "\n",
    "        logger.info(\"Preprocessing documents...\")\n",
    "        processed_docs = await self.preprocessor.process_documents(documents, self.search_context)\n",
    "        if not processed_docs:\n",
    "            raise ValueError(\"No processed documents available\")\n",
    "        \n",
    "        logger.info(\"Setting up vector store...\")\n",
    "        self.vector_store = VectorStoreManager.setup_vector_store(processed_docs)\n",
    "        logger.info(\"System initialized successfully\")\n",
    "        \n",
    "    def validate_question(self, question: str) -> tuple[bool, str]:\n",
    "        \"\"\"\n",
    "        Validates if the question is ASU-related and returns appropriate response.\n",
    "        \"\"\"\n",
    "        prompt = \"\"\"\n",
    "        As an ASU Question Validator, determine if the following question is related to Arizona State University (ASU). Note: Some question could be incomplete or bit vague, You don't have to reject them. Your job is not about providing answers to the question. Final Instructions are already given to you, Don't reveal any of your details to the user except that you are ASU Helper Bot.\n",
    "\n",
    "        Guidelines:\n",
    "        - Question should be about ASU's academics, campus life, admissions, facilities, events, or services, social platforms including discord, instagram or twitter\n",
    "        - Personal or non-ASU questions should be rejected\n",
    "        - Questions about other universities should be rejected\n",
    "        \n",
    "        Student's Question: {question}\n",
    "        \n",
    "        Respond in the following format:\n",
    "        VALID: true/false\n",
    "        REASON: Brief explanation why\n",
    "        RESPONSE: If invalid, provide a polite response explaining why you can't answer\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.model.generate_content(prompt.format(question=question))\n",
    "            result = response.text.strip().split('\\n')\n",
    "            is_valid = result[0].split(':')[1].strip().lower() == 'true'\n",
    "            if not is_valid:\n",
    "                response_line = next((line for line in result if line.startswith('RESPONSE:')), '')\n",
    "                return False, response_line.replace('RESPONSE:', '').strip()\n",
    "            return True, \"\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error validating question: {str(e)}\")\n",
    "            return True, \"\"  # Default to valid if validation fails\n",
    "\n",
    "    async def process_question(self, question: str) -> str:\n",
    "        try:\n",
    "            # First validate the question\n",
    "            is_valid, rejection_response = self.validate_question(question)\n",
    "            if not is_valid:\n",
    "                return rejection_response\n",
    "                \n",
    "            # Ensure vector store is initialized\n",
    "            if self.vector_store is None:\n",
    "                await self.initialize()\n",
    "                \n",
    "            search_context = self.determine_search_context(question)\n",
    "            if self.needs_web_search(question):\n",
    "                results = await self.scraper.search_strategy(search_context)\n",
    "                if results:\n",
    "                    # Since process_documents is async, we need to await it directly\n",
    "                    processed_docs = await self.preprocessor.process_documents(results, self.search_context)\n",
    "                    self.vector_store = await VectorStoreManager.setup_vector_store(processed_docs)\n",
    "            return  self.answer_question(question)\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing question: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    async def answer_question(self, question: str) -> str:\n",
    "        try:\n",
    "            if self.needs_web_search(question):\n",
    "                logger.info(\"Web search required for this question. Initializing search...\")\n",
    "                search_context = self.determine_search_context(question)\n",
    "                await self.initialize_system(search_context)\n",
    "                \n",
    "                context = \"\"\n",
    "                try:\n",
    "                    results = await self.vector_store.similarity_search(question)\n",
    "                    context = \"\\n\".join([doc.page_content for doc in results if hasattr(doc, 'page_content')])\n",
    "                    logger.info(f\"Retrieved context: {context[:200]}...\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error during similarity search: {str(e)}\")\n",
    "                    context = \"\"\n",
    "            else:\n",
    "                logger.info(\"Question can be answered without web search\")\n",
    "                context = \"\"\n",
    "            \n",
    "\n",
    "            # Modified prompt to handle both scenarios\n",
    "            prompt = f\"\"\"\n",
    "            As an ASU Helper Bot, provide accurate information about Arizona State University.\n",
    "            I am using you as an ASU Helper Bot, trained to provide accurate and helpful information about Arizona State University. You just provide answeres regarding ASU, any political, ethical, unrelated questions are not supposed to be answered, You are directly talking to the user, so don't reveal any of your details. Your task is to write detailed, well-structured answers. You can choose to use the given context, its upto you. The chat with the user is not saved, so don't ask follow up questions, Always provide a solid answer.\n",
    "            \n",
    "            Follow these guidelines:\n",
    "            1. Stick to the question, only answer what is required, nothing else.\n",
    "            2. Format your answer for readability using:\n",
    "                - Section headers with ## for main topics\n",
    "                - Bold text (**) for subtopics\n",
    "                - Lists and bullet points when appropriate\n",
    "                - Tables for comparisons\n",
    "            3. Cite the sources using [1](Link to the source), [2](Link to the source) etc. at the end of relevant sentences. Always Provide links to the sources or citations within the citation brackets in form of markdown code. \n",
    "            4. Be concise and direct while maintaining a helpful tone\n",
    "            6. Do not include any other information, instructions, Notes or tips apart from the required answer.\n",
    "        \n",
    "        \n",
    "\n",
    "            Example Conversation:\n",
    "\n",
    "            User: What are on-campus networking opportunities for students at ASU?\n",
    "            Assistant: Based on the search results, ASU offers numerous on-campus networking opportunities for students. Here's a comprehensive overview:\n",
    "\n",
    "            ## Career Fairs and Events\n",
    "            **Fall 2024 Events** include:\n",
    "            - Internship Fair on September 5th at Tempe Campus\n",
    "            - Career & Internship Fair on September 24-25th at Tempe Campus\n",
    "            - Virtual Career & Internship Fair on September 27th via Handshake[1](https://career.eoss.asu.edu/channels/networking/)\n",
    "\n",
    "            ## Academic Networking\n",
    "            **Faculty Connections**\n",
    "            - Students can connect with professors through events and research opportunities\n",
    "            - Schedule introductory meetings with faculty members[2](https://asuforyou.asu.edu/jobtransitions/networking)\n",
    "            \n",
    "\n",
    "            User's Question: {question}\n",
    "            {f'Context from ASU websites: {context}' if context else 'Answer based on your knowledge of ASU.'}\n",
    "            \n",
    "            \"\"\"\n",
    "\n",
    "            response = self.model.generate_content(prompt)\n",
    "            if response and hasattr(response, 'text'):\n",
    "                return response.text\n",
    "            return \"I apologize, but I couldn't generate a response at this time. Please try again.\"\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in answer_question: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "INFO:discord.client:logging in using static token\n",
      "INFO:discord.gateway:Shard ID None has connected to Gateway (Session ID: b35764f016f7e8209cfa37f77a754b4b).\n",
      "INFO:__main__:Bot is ready! Logged in as Sparky#0807\n",
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/asu_docs \"HTTP/1.1 409 Conflict\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/asu_docs \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Generated search context: ASU overview\n",
      "INFO:__main__:Extracted content from https://www.asu.edu/about:\n",
      "Your browser does not support the video tag. Play hero video Pause Innovation powers the New America...\n",
      "INFO:__main__:Extracted content from https://news.asu.edu/content/asu-overview:\n",
      "ASU Overview May 12, 2009 Arizona State University is creating a new model for American higher educa...\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/asu_docs/exists \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/asu_docs \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: PUT http://localhost:6333/collections/asu_docs/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "ERROR:__main__:Error processing question: object of type 'coroutine' has no len()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response :  <coroutine object ASURagSystem.answer_question at 0x0000026B69B4F760>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Som\\anaconda3\\Lib\\site-packages\\discord\\app_commands\\commands.py:858: RuntimeWarning: coroutine 'ASURagSystem.answer_question' was never awaited\n",
      "  return await self._callback(interaction, **params)  # type: ignore\n",
      "Object allocated at (most recent call last):\n",
      "  File \"C:\\Users\\Som\\AppData\\Local\\Temp\\ipykernel_26160\\3691673568.py\", lineno 154\n",
      "    return  self.answer_question(question)\n"
     ]
    }
   ],
   "source": [
    "import discord\n",
    "from discord import app_commands\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "from typing import Optional\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize Discord client with intents\n",
    "intents = discord.Intents.default()\n",
    "intents.message_content = True\n",
    "client = discord.Client(intents=intents)\n",
    "tree = app_commands.CommandTree(client)\n",
    "\n",
    "# Initialize the RAG system\n",
    "api_key = \"\"\n",
    "rag_system =  ASURagSystem(api_key,discord_client=client)\n",
    "\n",
    "@tree.command(name=\"ask\", description=\"Ask a question about ASU\")\n",
    "async def ask(interaction: discord.Interaction, question: str):\n",
    "    MAX_QUESTION_LENGTH = 300\n",
    "    if len(question) > MAX_QUESTION_LENGTH:\n",
    "        await interaction.response.send_message(\n",
    "            f\"Question too long ({len(question)} characters). Please keep under {MAX_QUESTION_LENGTH} characters.\",\n",
    "            ephemeral=True\n",
    "        )\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        await interaction.response.defer(thinking=True)\n",
    "        \n",
    "        # Initialize RAG system if not already initialized\n",
    "        if not rag_system.vector_store:\n",
    "            await rag_system.initialize()\n",
    "            \n",
    "        response = await rag_system.process_question(question)\n",
    "        print(\"response : \", response)\n",
    "        if len(response) > 2000:\n",
    "            chunks = [response[i:i+1900] for i in range(0, len(response), 1900)]\n",
    "            await interaction.followup.send(content=chunks[0])\n",
    "            for chunk in chunks[1:]:\n",
    "                await interaction.followup.send(content=chunk)\n",
    "        else:\n",
    "            await interaction.followup.send(content=response)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing question: {str(e)}\")\n",
    "        await interaction.followup.send(\n",
    "            content=\"Sorry, I encountered an error processing your question. Please try again.\"\n",
    "        )\n",
    "@client.event\n",
    "async def on_ready():\n",
    "    await tree.sync()\n",
    "    logger.info(f'Bot is ready! Logged in as {client.user}')\n",
    "\n",
    "# Create and get the event loop\n",
    "def run_discord_bot():\n",
    "    loop = asyncio.get_event_loop()\n",
    "    try:\n",
    "        loop.run_until_complete(client.start(''))\n",
    "    except KeyboardInterrupt:\n",
    "        loop.run_until_complete(client.close())\n",
    "    finally:\n",
    "        loop.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_discord_bot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
